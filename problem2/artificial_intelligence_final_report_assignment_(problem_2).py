# -*- coding: utf-8 -*-
"""Artificial Intelligence Final Report Assignment (Problem 2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DDyh1NUSVJOpP78Qy3ifUT_N1vzC7NG4

##The program before improvement.
"""

!pip install torchdata
!pip install portalocker

import torch
import torch.nn.functional as F
import torchtext

train_iter, test_iter = torchtext.datasets.IMDB(split=('train', 'test'))
tokenizer = torchtext.data.utils.get_tokenizer('basic_english')
MODELNAME = "imdb-rnn.model"
EPOCH = 10
BATCHSIZE = 100
LR = 1e-5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)

train_data = [(label, tokenizer(line)) for label, line in train_iter]
train_data.sort(key=lambda x: len(x[1]))
test_data = [(label, tokenizer(line)) for label, line in test_iter]
test_data.sort(key=lambda x: len(x[1]))

def make_vocab(train_data, min_freq):
    vocab = {}
    for label, tokenlist in train_data:
        for token in tokenlist:
            if token not in vocab:
                vocab[token] = 0
            vocab[token] += 1
    vocablist = [('<unk>', 0), ('<pad>', 0), ('<cls>', 0), ('<eos>', 0)]
    vocabidx = {}
    for token, freq in vocab.items():
        if freq >= min_freq:
            idx = len(vocablist)
            vocablist.append((token, freq))
            vocabidx[token] = idx
    vocabidx['<unk>'] = 0
    vocabidx['<pad>'] = 1
    vocabidx['<cls>'] = 2
    vocabidx['<eos>'] = 3
    return vocablist, vocabidx

vocablist, vocabidx = make_vocab(train_data, 10)

def preprocess(data, vocabidx):
    processed_data = []
    for label, tokenlist in data:
        processed_tokenlist = ['<cls>']
        for token in tokenlist:
            processed_tokenlist.append(token if token in vocabidx else '<unk>')
        processed_tokenlist.append('<eos>')
        processed_data.append((label, processed_tokenlist))
    return processed_data

train_data = preprocess(train_data, vocabidx)
test_data = preprocess(test_data, vocabidx)

def make_batch(data, batchsize):
    batches = []
    batch_labels = []
    batch_tokenlists = []
    for label, tokenlist in data:
        batch_labels.append(label)
        batch_tokenlists.append(tokenlist)
        if len(batch_labels) >= batchsize:
            batches.append((batch_tokenlists, batch_labels))
            batch_labels = []
            batch_tokenlists = []
    if len(batch_labels) > 0:
        batches.append((batch_tokenlists, batch_labels))
    return batches

train_data = make_batch(train_data, BATCHSIZE)
test_data = make_batch(test_data, BATCHSIZE)

print(len(train_data[0][0]))

def padding(batches):
    for tokenlists, labels in batches:
        maxlen = max([len(x) for x in tokenlists])
        for tkl in tokenlists:
            tkl.extend(['<pad>'] * (maxlen - len(tkl)))
    return batches

train_data = padding(train_data)
test_data = padding(test_data)

def word2id(batches, vocabidx):
    processed_batches = []
    for tokenlists, labels in batches:
        id_labels = [label - 1 for label in labels]
        id_tokenlists = [[vocabidx[token] for token in tokenlist] for tokenlist in tokenlists]
        processed_batches.append((id_tokenlists, id_labels))
    return processed_batches

train_data = word2id(train_data, vocabidx)
test_data = word2id(test_data, vocabidx)

class MyRNN(torch.nn.Module):
    def __init__(self):
        super(MyRNN, self).__init__()
        vocabsize = len(vocablist)
        self.emb = torch.nn.Embedding(vocabsize, 300, padding_idx=vocabidx['<pad>'])
        self.l1 = torch.nn.Linear(300, 300)
        self.l2 = torch.nn.Linear(300, 2)

    def forward(self, x):
        e = self.emb(x)
        h = torch.zeros(e[0].size(), dtype=torch.float32).to(DEVICE)
        for i in range(x.size()[0]):
            h = F.relu(e[i] + self.l1(h))
        return self.l2(h)

import torch.nn as nn

def train():
    model = MyRNN().to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)
    best_accuracy = 0.0
    no_improvement_count = 0

    for epoch in range(EPOCH):
        running_loss = 0.0
        correct = 0
        total = 0

        for i, (tokenlists, labels) in enumerate(train_data):
            tokenlists = torch.tensor(tokenlists, dtype=torch.int64).transpose(0, 1).to(DEVICE)
            labels = torch.tensor(labels, dtype=torch.int64).to(DEVICE)

            optimizer.zero_grad()
            outputs = model(tokenlists)
            loss = F.cross_entropy(outputs, labels)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            if (i + 1) % 10 == 0:
                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %
                      (epoch + 1, EPOCH, i + 1, len(train_data), running_loss / 10))
                running_loss = 0.0

        accuracy = correct / total
        print('Accuracy on epoch %d: %.2f %%' % (epoch + 1, 100 * accuracy))

        scheduler.step(accuracy)  # Learning rate schedule

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            no_improvement_count = 0
            torch.save(model.state_dict(), MODELNAME)
        else:
            no_improvement_count += 1
            if no_improvement_count >= 5:  # Early stopping
                print("No improvement in accuracy for 5 epochs. Training stopped.")
                break

    print("Training completed.")

train()

def test():
    total = 0
    correct = 0
    model = MyRNN().to(DEVICE)
    model.load_state_dict(torch.load(MODELNAME))
    model.eval()
    for tokenlists, labels in test_data:
        total += len(labels)
        tokenlists = torch.tensor(tokenlists, dtype=torch.int64).transpose(0, 1).to(DEVICE)
        labels = torch.tensor(labels, dtype=torch.int64).to(DEVICE)
        y = model(tokenlists)
        pred_labels = y.max(dim=1)[1]
        correct += (pred_labels == labels).sum()

    print("correct:", correct.item())
    print("total:", total)
    print("accuracy:", (correct.item() / float(total)))

test()

